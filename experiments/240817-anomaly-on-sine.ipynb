{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related Post: [Anomaly detection with HTM.core model on sine](https://discourse.numenta.org/t/anomaly-detection-with-htm-core-model-on-sine/8975/4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "from htm.encoders.scalar_encoder import ScalarEncoder, ScalarEncoderParameters\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.algorithms.anomaly_likelihood import \\\n",
    "    AnomalyLikelihood  # FIXME use TM.anomaly instead, but it gives worse results than the py.AnomalyLikelihood now\n",
    "from htm.bindings.algorithms import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m8640\u001b[39m)\n\u001b[1;32m     20\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mzip\u001b[39m(x, y, \u001b[43mdate\u001b[49m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# making label\u001b[39;00m\n\u001b[1;32m     25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, np.pi*60, 8640)\n",
    "y = np.sin(x) + 1\n",
    "\n",
    "df = pd.DataFrame(data=zip(x, y, date), index=range(len(x)), columns=['x','y', 'date'])\n",
    "\n",
    "# making label\n",
    "df['label'] = 0\n",
    "\n",
    "df.iloc[3700:3800,3] = 1\n",
    "df.iloc[4100:4150,3] = 1\n",
    "df.iloc[6000:6100,3] = 1\n",
    "df.iloc[6800:7000,3] = 1\n",
    "\n",
    "# making the anomaly\n",
    "df.iloc[3700:3800,1] = 0.7\n",
    "df.iloc[4100:4150,1] = 0.5\n",
    "df.iloc[6000:6100,1] = 0.0\n",
    "df.iloc[6800:7000,1] = np.random.random_sample([200,1])\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "parameters = {\n",
    "    'predictor': {'sdrc_alpha': 0.05},\n",
    "    'sp': { \n",
    "            'columnDimensions': (2048, ),\n",
    "            'potentialRadius' : 2048,\n",
    "            'globalInhibition' : True\n",
    "          },\n",
    "    'tm': {\n",
    "        'columnDimensions': (2048, ), \n",
    "        'activationThreshold': 17,\n",
    "        'cellsPerColumn': 13,\n",
    "        'initialPerm': 0.21,\n",
    "        'maxSegmentsPerCell': 128,\n",
    "        'maxSynapsesPerSegment': 64,\n",
    "        'minThreshold': 10,\n",
    "        'connectedPermanence': 0.13999999999999999,\n",
    "        'newSynapseCount': 32,\n",
    "        'permanenceDec': 0.1,\n",
    "        'permanenceInc': 0.1},\n",
    "    'anomaly': {\n",
    "        'likelihood':\n",
    "            { \n",
    "            'learningPeriod':400, # if None it will be calculated later, else value (indication is 500 for 5-min interval)\n",
    "            'historicWindowSize':4000,  # default of 8640 is a month's worth of history at 5-minute intervals\n",
    "            'probationaryPct': 0.1,\n",
    "            'reestimationPeriod': 100,\n",
    "            'estimationSamples':100}  # how often we re-estimate the Gaussian distribution\n",
    "    }\n",
    "}\n",
    "\n",
    "#when changing the encoders\n",
    "par = ScalarEncoderParameters()\n",
    "par.activeBits = 24\n",
    "par.minimum = -1\n",
    "par.maximum = 3\n",
    "par.size = 2048*2\n",
    "scalarEncoder = ScalarEncoder(par)\n",
    "\n",
    "dateEncoder = DateEncoder(timeOfDay=(7,4), weekend=3, dayOfWeek=7)\n",
    "encodingWidth = (dateEncoder.size + scalarEncoder.size)\n",
    "enc_info = Metrics([encodingWidth], 999999999)\n",
    "\n",
    "spParams = parameters[\"sp\"]\n",
    "\n",
    "sp = SpatialPooler(\n",
    "        inputDimensions=(encodingWidth,),\n",
    "        potentialRadius = spParams[\"potentialRadius\"],\n",
    "        globalInhibition = spParams[\"globalInhibition\"],\n",
    "        columnDimensions=spParams[\"columnDimensions\"])\n",
    "\n",
    "sp_info = Metrics(sp.getColumnDimensions(), 999999999)\n",
    "\n",
    "tmParams = parameters[\"tm\"]\n",
    "\n",
    "# tm = TemporalMemory(\n",
    "#         columnDimensions=tmParams[\"columnDimensions\"])\n",
    "tm = TemporalMemory(\n",
    "        columnDimensions=tmParams[\"columnDimensions\"],\n",
    "        cellsPerColumn=tmParams[\"cellsPerColumn\"],\n",
    "        activationThreshold=tmParams[\"activationThreshold\"],\n",
    "        initialPermanence=tmParams[\"initialPerm\"],\n",
    "        connectedPermanence=tmParams[\"connectedPermanence\"],\n",
    "        minThreshold=tmParams[\"minThreshold\"],\n",
    "        maxNewSynapseCount=tmParams[\"newSynapseCount\"],\n",
    "        permanenceIncrement=tmParams[\"permanenceInc\"],\n",
    "        permanenceDecrement=tmParams[\"permanenceDec\"],\n",
    "        predictedSegmentDecrement=0.0,\n",
    "        maxSegmentsPerCell=tmParams[\"maxSegmentsPerCell\"],\n",
    "        maxSynapsesPerSegment=tmParams[\"maxSynapsesPerSegment\"]\n",
    "        )\n",
    "\n",
    "tm_info = Metrics([tm.numberOfCells()], 999999999)\n",
    "\n",
    "step = 5\n",
    "# setup likelihood, these settings are used in NAB\n",
    "anParams = parameters[\"anomaly\"][\"likelihood\"]\n",
    "if anParams.get(\"learningPeriod\") is None:\n",
    "    probationaryPeriod = int(math.floor(float(anParams[\"probationaryPct\"]) * df.shape[0]))\n",
    "    learningPeriod = int(math.floor(probationaryPeriod / 2.0))\n",
    "else:\n",
    "    learningPeriod = anParams[\"learningPeriod\"]\n",
    "anomaly_history = AnomalyLikelihood(learningPeriod=learningPeriod,\n",
    "                                    estimationSamples=anParams[\"estimationSamples\"],\n",
    "                                    reestimationPeriod=anParams[\"reestimationPeriod\"],\n",
    "                                    historicWindowSize=anParams[\"historicWindowSize\"])\n",
    "\n",
    "predictor = Predictor(steps=[1, step], alpha=parameters[\"predictor\"]['sdrc_alpha'])\n",
    "predictor_resolution = 0.1\n",
    "\n",
    "inputs = []\n",
    "anomaly = []\n",
    "anomalyLikelihood = []\n",
    "log_anomalyLikelihood = []\n",
    "predictions = {1: [], step: []}\n",
    "for count, record in df.iterrows():\n",
    "\n",
    "    dateBits = dateEncoder.encode(record.date)\n",
    "    consumptionBits = scalarEncoder.encode(record.y)\n",
    "\n",
    "    # Concatenate all these encodings into one large encoding for Spatial Pooling.\n",
    "    encoding = SDR(encodingWidth).concatenate([consumptionBits, dateBits])\n",
    "    enc_info.addData(encoding)\n",
    "\n",
    "    # Create an SDR to represent active columns, This will be populated by the\n",
    "    # compute method below. It must have the same dimensions as the Spatial Pooler.\n",
    "    activeColumns = SDR(sp.getColumnDimensions())\n",
    "\n",
    "    # Execute Spatial Pooling algorithm over input space.\n",
    "    overlaps = sp.compute(encoding, True, activeColumns)\n",
    "        \n",
    "    sp_info.addData(activeColumns)\n",
    "    \n",
    "    # Execute Temporal Memory algorithm over active mini-columns.\n",
    "    tm.compute(activeColumns, learn=True)\n",
    "    tm_info.addData(tm.getActiveCells().flatten())\n",
    "\n",
    "    # Predict what will happen, and then train the predictor based on what just happened.\n",
    "    pdf = predictor.infer(tm.getActiveCells())\n",
    "    for n in (1, step):\n",
    "        if pdf[n]:\n",
    "            predictions[n].append(np.argmax(pdf[n]) * predictor_resolution)\n",
    "        else:\n",
    "            predictions[n].append(float('nan'))\n",
    "\n",
    "    anomaly_Likelihood = anomaly_history.anomalyProbability(record.y, tm.anomaly)\n",
    "    anomaly.append(tm.anomaly)\n",
    "    logAnomalyLikelihood = np.log(1.0000000001 - anomaly_Likelihood) / -23.02585084720009\n",
    "    anomalyLikelihood.append(anomaly_Likelihood)\n",
    "    log_anomalyLikelihood.append(logAnomalyLikelihood)\n",
    "    \n",
    "    predictor.learn(count, tm.getActiveCells(), int(record.y / predictor_resolution))\n",
    "\n",
    "# Print information & statistics about the state of the HTM.\n",
    "print(\"Encoded Input\", enc_info)\n",
    "print(\"\")\n",
    "print(\"Spatial Pooler Mini-Columns\", sp_info)\n",
    "print(str(sp))\n",
    "print(\"\")\n",
    "print(\"Temporal Memory Cells\", tm_info)\n",
    "print(str(tm))\n",
    "print(\"\")\n",
    "\n",
    "THRESHOLD_LIKELIHOOD = 0.3\n",
    "THRESHOLD_RAW_SCORE = 0.9\n",
    "df['loglikelihood_anomaly'] = log_anomalyLikelihood\n",
    "df['raw_anomaly_score'] = anomaly\n",
    "\n",
    "# Shift the predictions so that they are aligned with the input they predict.\n",
    "for n_steps, pred_list in predictions.items():\n",
    "    for i in range(n_steps):\n",
    "        pred_list.insert(0, float('nan'))\n",
    "        pred_list.pop()\n",
    "\n",
    "# Calculate the predictive accuracy, Root-Mean-Squared\n",
    "accuracy = {1: 0, step: 0}\n",
    "accuracy_samples = {1: 0, step: 0}\n",
    "\n",
    "for idx, inp in enumerate(df.y):\n",
    "    for n in predictions:  # For each [N]umber of time steps ahead which was predicted.\n",
    "        val = predictions[n][idx]\n",
    "        if not np.isnan(val):\n",
    "            accuracy[n] += (inp - val) ** 2\n",
    "            accuracy_samples[n] += 1\n",
    "for n in sorted(predictions):\n",
    "    accuracy[n] = (accuracy[n] / accuracy_samples[n]) ** .5\n",
    "    print(\"Predictive Error (RMS)\", n, \"steps ahead:\", accuracy[n])\n",
    "print(\"Random guess, mean temperature:\")\n",
    "print(mean_squared_error(df.y, [np.mean(df.y)]*df.y.shape[0])**0.5)\n",
    "\n",
    "df_t = df[(df.raw_anomaly_score >=THRESHOLD_RAW_SCORE)] # (df.loglikelihood_anomaly >=THRESHOLD_LIKELIHOOD) | \n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "fig.add_trace(go.Scatter(x=df.index, y=df.y, name='Sinus'), secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_t.index, y=df_t.y, mode='markers', name='Anomaly'), secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df.index, y=predictions[1], name='prediction one step ahead'), secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df.index, y=predictions[step], name=f'prediction {step} step ahead'), secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df.index, y=anomaly, name='Anomaly score TM'), secondary_y=True)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df.index, y=anomalyLikelihood, name='Anomaly Likelihood'), secondary_y=True)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df.index, y=log_anomalyLikelihood, name='Log Likelihood', line_color='#ffe476'), secondary_y=True)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df.index, y=np.array([THRESHOLD_LIKELIHOOD]*df.shape[0]), name='Threshold_Likelihood'), secondary_y=True)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=np.array([THRESHOLD_RAW_SCORE]*df.shape[0]), name='Threshold_Raw_score'), secondary_y=True)\n",
    "\n",
    "\n",
    "fig.update_layout(autosize=False, width=1000, height=500)\n",
    "fig.update_yaxes(\n",
    "    title_text = \"Sinus\",\n",
    "    title_standoff = 25,\n",
    "    secondary_y=False)\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text = \"Anomaly score\",\n",
    "    title_standoff = 25,\n",
    "    secondary_y=True)\n",
    "\n",
    "for i in [[3700,3800], [4100,4150], [6000,6100], [6800,7000]]:\n",
    "    fig.add_vrect(\n",
    "        x0=i[0], x1=i[1],\n",
    "        fillcolor=\"LightSalmon\", opacity=0.5,\n",
    "        layer=\"below\", line_width=0,\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
