{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm.bindings.sdr import SDR, Metrics\n",
    "# from htm.encoders.scalar_encoder import ScalarEncoder, ScalarEncoderParameters\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import datetime\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "import time\n",
    "import traceback\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexiveMemory:\n",
    "  def __init__(self, dimensions):\n",
    "    self.acKey0 = None\n",
    "    self.pairs = {}\n",
    "    self.dimensions = dimensions\n",
    "    self.anomaly = []\n",
    "    self.enableLearn = False\n",
    "    self.rm_time  = 0\n",
    "\n",
    "\n",
    "  def add(self, activeColumns):\n",
    "    acKey1 = '-'.join(map(str, activeColumns.sparse))\n",
    "    if(self.acKey0 != None):\n",
    "\n",
    "      sequence = self.pairs.get(self.acKey0, {})\n",
    "      sequence_data = sequence.get(acKey1, {\n",
    "         \"count\": 0,\n",
    "         \"time\": datetime.now()\n",
    "      })\n",
    "      sequence_data[\"count\"] = sequence_data[\"count\"] + 1\n",
    "      sequence_data[\"time\"] = datetime.now()\n",
    "\n",
    "      if self.pairs.get(self.acKey0, None) is None:\n",
    "        self.pairs[self.acKey0] = { acKey1: sequence_data }\n",
    "      else:\n",
    "        self.pairs[self.acKey0][acKey1] = sequence_data\n",
    "\n",
    "      table_size = 0\n",
    "      oldKey1 = None\n",
    "      oldKey2 = None\n",
    "      oldTime = datetime.now()\n",
    "      for key1, value1 in self.pairs.items():\n",
    "        table_size = table_size + len(value1.items())\n",
    "        for key2, value2 in value1.items():\n",
    "          if value2['time'] < oldTime:\n",
    "            oldKey1 = key1\n",
    "            oldKey2 = key2\n",
    "            oldTime = datetime.now()\n",
    "      if table_size > 127:\n",
    "        self.enableLearn = True\n",
    "        del self.pairs[oldKey1][oldKey2]\n",
    "        if len(self.pairs[oldKey1].items()) == 0:\n",
    "          del self.pairs[oldKey1]\n",
    "\n",
    "    self.acKey0 = acKey1\n",
    "\n",
    "  def predict(self, activeColumns):\n",
    "    return_count = 0\n",
    "    return_sdr = None\n",
    "\n",
    "    acKey = '-'.join(map(str, activeColumns.sparse))\n",
    "    sequences = self.pairs.get(acKey, {})\n",
    "    for sequence_key, sequence_data in sequences.items():\n",
    "      if sequence_data[\"count\"] > return_count:\n",
    "        return_count = sequence_data[\"count\"]\n",
    "        return_sdr = sequence_key\n",
    "\n",
    "    if return_sdr is not None:\n",
    "      tmp_sdr = SDR( self.dimensions )\n",
    "      tmp_sdr.sparse = list(map(int, return_sdr.split('-')))\n",
    "      return_sdr = tmp_sdr\n",
    "\n",
    "    return return_count, return_sdr\n",
    "\n",
    "  # Control Unit\n",
    "  def learn(self, activeColumns1, tm):\n",
    "    pred_correct = False\n",
    "    pred_anomaly = None\n",
    "\n",
    "    if(self.acKey0 is not None):\n",
    "\n",
    "        activeColumns0 = SDR( self.dimensions )\n",
    "        activeColumns0.sparse = list(map(int, self.acKey0.split('-')))\n",
    "\n",
    "        tm.activateDendrites(True)\n",
    "        predictiveColumns = SDR( self.dimensions )\n",
    "        predictiveColumns.sparse = list(set(sorted(list(np.where(tm.getPredictiveCells().dense == 1)[0]))))\n",
    "\n",
    "\n",
    "        rm_start = time.time()\n",
    "        reflexiveCount, reflexiveColumns = self.predict(activeColumns0)\n",
    "        rm_end = time.time()\n",
    "        self.rm_time = self.rm_time + (rm_end- rm_start)\n",
    "\n",
    "\n",
    "        if reflexiveColumns is not None:\n",
    "            \n",
    "            pred_anomaly = 1 - np.count_nonzero((reflexiveColumns.dense & activeColumns1.dense)) / np.count_nonzero(activeColumns1.dense)\n",
    "            \n",
    "            # RM-1 SM-?\n",
    "            if activeColumns1.flatten() == reflexiveColumns.flatten():\n",
    "                pred_correct = True\n",
    "                pred_anomaly = 0\n",
    "\n",
    "            # RM-0 SM-1\n",
    "            elif activeColumns1.flatten() == predictiveColumns.flatten():\n",
    "                key1 = self.acKey0\n",
    "                key2 = '-'.join(map(str, reflexiveColumns.sparse))\n",
    "                self.pairs[key1][key2][\"count\"] = reflexiveCount - 1\n",
    "                if self.pairs[key1][key2][\"count\"] < 1:\n",
    "                  self.pairs[key1][key2][\"count\"] = 1\n",
    "\n",
    "                key2 = '-'.join(map(str, predictiveColumns.sparse))\n",
    "                key2_data = self.pairs.get(key1, {}).get(key2, {\n",
    "                  \"count\": 1,\n",
    "                  \"time\": datetime.now()\n",
    "                })\n",
    "                self.pairs[key1][key2] = key2_data\n",
    "\n",
    "            # RM-0 SM-0\n",
    "            else:\n",
    "                key1 = self.acKey0\n",
    "                key2 = '-'.join(map(str, reflexiveColumns.sparse))\n",
    "                self.pairs[key1][key2][\"count\"] = reflexiveCount - 1\n",
    "                if self.pairs[key1][key2][\"count\"] < 1:\n",
    "                  self.pairs[key1][key2][\"count\"] = 1\n",
    "\n",
    "\n",
    "    self.anomaly.append( pred_anomaly )\n",
    "\n",
    "  def compute(self, activeColumns, tm):\n",
    "    if self.enableLearn:\n",
    "      self.learn(activeColumns, tm)  \n",
    "    self.add(activeColumns)\n",
    "  \n",
    "  def save_to_csv(self, dataset_name, time_with_rm):\n",
    "\n",
    "    save_dir='./saved_reflex_data/'\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a filename based on the dataset name, in the specified directory\n",
    "    filename = os.path.join(save_dir, \"Sabrina_4CAM_delay_times.xlsx\")\n",
    "  \n",
    "    # filename = os.path.join(save_dir, f\"{dataset_name}_reflex_memory.csv\")\n",
    "    \n",
    "    # # Save self.pairs to a CSV file\n",
    "    # with open(filename, 'w', newline='') as f:\n",
    "    #   writer = csv.writer(f)\n",
    "    #   # writer.writerow(['Key (1024 bits)', 'Values (1024 bits)'])\n",
    "        \n",
    "    #     # Write each key (as 1024 bits) and all corresponding values (also 1024 bits each)\n",
    "    # print(\"The name of the dataset: \",dataset_name)  \n",
    "    # print(\"No of keys: \", len(self.pairs.items()))\n",
    "    valueCount = []\n",
    "    frequencyCount = []\n",
    "    for key1, value1 in self.pairs.items():\n",
    "        valueCount.append(len(value1.items()))\n",
    "        \n",
    "        # print(\"\\n\"+hashlib.md5(key1.encode()).hexdigest(), len(value1.items()))\n",
    "        for key2, value2 in value1.items():\n",
    "            result = hashlib.md5(key1.encode())\n",
    "            # print(result.hexdigest(), end=' ')\n",
    "            result = hashlib.md5(key2.encode())\n",
    "            # print(result.hexdigest(), end=' ')\n",
    "            # print(value2[\"count\"], end=' ')\n",
    "            # print(value2[\"time\"].timestamp())\n",
    "            frequencyCount.append(value2[\"count\"])\n",
    "\n",
    "    # print(\"Values count per key: \",valueCount)\n",
    "    # print(\"Highest Value count: \", max(valueCount))\n",
    "    # print(\"Frequency count: \", frequencyCount)\n",
    "    # print(f\"Time taken with Reflexive Memory: {time_with_rm:.2f} seconds\")\n",
    "    # print(\"length\",len(frequencyCount))\n",
    "    \n",
    "\n",
    "    \n",
    "    # Data to be written into the Excel file\n",
    "    data = {\n",
    "        \"Dataset Name\": [dataset_name],\n",
    "        \"No of Keys\": [len(self.pairs.items())],\n",
    "        \"Values Count per Key\": [valueCount],\n",
    "        \"Highest Value Count\": [max(valueCount)],\n",
    "        \"Frequency count\": [frequencyCount],\n",
    "        \"Time Taken with Reflexive Memory (seconds)\": [time_with_rm]\n",
    "    }\n",
    "\n",
    "    # Convert the new data into a pandas DataFrame\n",
    "    new_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Check if the Excel file already exists\n",
    "    if os.path.exists(filename):\n",
    "        # If the file exists, load the existing data\n",
    "        existing_df = pd.read_excel(filename)\n",
    "        # Append the new data to the existing DataFrame\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        # If the file does not exist, the new data is the only data\n",
    "        updated_df = new_df\n",
    "\n",
    "    # Write the updated DataFrame to the Excel file\n",
    "    updated_df.to_excel(filename, index=False)\n",
    "\n",
    "    print(\"Data written to\", filename)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSources = [\n",
    "    # \"hourly_numentaTM_speed_7578.csv\",\n",
    "    # \"hourly_numentaTM_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\",\n",
    "    # \"hourly_numentaTM_exchange-3_cpc_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-3_cpm_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-2_cpc_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-2_cpm_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-4_cpc_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-4_cpm_results.csv\",\n",
    "    # \"hourly_numentaTM_rogue_agent_key_hold.csv\",\n",
    "    # \"hourly_numentaTM_TravelTime_451.csv\",\n",
    "    # \"hourly_numentaTM_occupancy_6005.csv\",\n",
    "    # \"hourly_numentaTM_speed_t4013.csv\",\n",
    "    # \"hourly_numentaTM_TravelTime_387.csv\",\n",
    "    # \"hourly_numentaTM_occupancy_t4013.csv\",\n",
    "    # \"hourly_numentaTM_speed_6005.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_flatmiddle.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_jumpsdown.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_jumpsup.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_no_noise.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_nojump.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_perfect_square_wave.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_small_noise.csv\",\n",
    "    # \"hourly_numentaTM_art_flatline.csv\",\n",
    "    # \"hourly_numentaTM_art_increase_spike_density.csv\",\n",
    "    # \"hourly_numentaTM_art_load_balancer_spikes.csv\",\n",
    "    # \"hourly_numentaTM_art_noisy.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_24ae8d.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_53ea38.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_5f5533.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_77c1ca.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_825cc2.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_ac20cd.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_c6585a.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_fe7f93.csv\",\n",
    "    # \"hourly_numentaTM_ec2_disk_write_bytes_c0d644.csv\",\n",
    "    # \"hourly_numentaTM_ec2_network_in_257a54.csv\",\n",
    "    # \"hourly_numentaTM_ec2_request_latency_system_failure.csv\",\n",
    "    # \"hourly_numentaTM_elb_request_count_8c0756.csv\",\n",
    "    # \"hourly_numentaTM_rds_cpu_utilization_cc0c53.csv\",\n",
    "    # \"hourly_numentaTM_rds_cpu_utilization_e47b3b.csv\",\n",
    "    # \"hourly_numentaTM_grok_asg_anomaly.csv\",\n",
    "    # \"hourly_numentaTM_ec2_disk_write_bytes_1ef3de.csv\",\n",
    "    # \"hourly_numentaTM_ec2_network_in_5abac7.csv\",\n",
    "    # \"hourly_numentaTM_rogue_agent_key_updown.csv\",\n",
    "    # \"hourly_numentaTM_ambient_temperature_system_failure.csv\",\n",
    "    # \"hourly_numentaTM_nyc_taxi.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_AMZN.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_FB.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_GOOG.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_KO.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_CVS.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_PFE.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_UPS.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_IBM.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_AAPL.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_CRM.csv\",\n",
    "    # \"hourly_numentaTM_cpu_utilization_asg_misconfiguration.csv\",\n",
    "    # \"hourly_numentaTM_machine_temperature_system_failure.csv\",\n",
    "\n",
    "\n",
    "#    \"value1_pseudo_periodic_synthetic_1.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_2.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_3.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_4.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_5.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_6.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_7.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_8.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_9.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_10.csv\",\n",
    "#    \"monthly_gold_prices.csv\",\n",
    "   \"monthly_sp500.csv\",\n",
    "   \"weekly_dow_jones.csv\",\n",
    "   \"weekly_nasdaq.csv\",\n",
    "   \"weekly_sp500.csv\",\n",
    "   \"monthly_vix_close.csv\",\n",
    "   \"monthly_vix_high.csv\",\n",
    "   \"monthly_vix_low.csv\",\n",
    "   \"monthly_vix_open.csv\",\n",
    "   \"daily_natural_gas.csv\",\n",
    "   \"daily_oil_prices.csv\",\n",
    "   \"value1_vix_close.csv\",\n",
    "   \"value1_vix_high.csv\",\n",
    "   \"value1_vix_low.csv\",\n",
    "   \"value1_vix_open.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_date(date_str):\n",
    "    formats = {\n",
    "        7: \"%Y-%m\",                    # Format: yyyy-mm\n",
    "        10: \"%Y-%m-%d\",                # Format: yyyy-mm-dd\n",
    "        19: \"%Y-%m-%d %H:%M:%S\"        # Format: yyyy-mm-dd hh-mm-ss\n",
    "    }\n",
    "\n",
    "    date_format = formats.get(len(date_str))\n",
    "\n",
    "    if date_format:\n",
    "        # Use `datetime.strptime` directly\n",
    "        return datetime.strptime(date_str, date_format)\n",
    "    else:\n",
    "        raise ValueError(f\"Date format not recognized for: {date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'enc': {\n",
    "        \"value\" :\n",
    "            {'resolution': 0.88, 'size': 700, 'sparsity': 0.02},\n",
    "        \"time\": \n",
    "            {'timeOfDay': (30, 1), 'weekend': 21}\n",
    "    },\n",
    "    'sp': {\n",
    "        'inputDimensions': None,\n",
    "        'columnDimensions': (1638,),\n",
    "        'potentialPct': 0.85,\n",
    "        'potentialRadius': None,\n",
    "        'globalInhibition': True,\n",
    "        'localAreaDensity': 0.04395604395604396,\n",
    "        'synPermInactiveDec': 0.006,\n",
    "        'synPermActiveInc': 0.04,\n",
    "        'synPermConnected': 0.13999999999999999,\n",
    "        'boostStrength': 3.0,\n",
    "        'wrapAround': True,\n",
    "        'seed': 1,\n",
    "        'learn': False,\n",
    "    },\n",
    "    'tm': {\n",
    "        'cellsPerColumn': 13,\n",
    "        'activationThreshold': 17,\n",
    "        'initialPermanence': 0.21,\n",
    "        'minThreshold': 10,\n",
    "        'maxNewSynapseCount': 32,\n",
    "        'permanenceIncrement': 0.1,\n",
    "        'permanenceDecrement': 0.1,\n",
    "        'predictedSegmentDecrement': 0.0,\n",
    "        'maxSegmentsPerCell': 128,\n",
    "        'maxSynapsesPerSegment': 64,\n",
    "        'learn': True\n",
    "    },\n",
    "    'anomaly': {'period': 1000},\n",
    "    'learnRows': 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monthly_sp500.csv\n",
      "1765\n",
      "Time Comparison: 0.45191240310668945 and 0.7285416126251221 seconds\n",
      "The anomaly scores are: 741 and 1766\n",
      "\n",
      "weekly_dow_jones.csv\n",
      "2079\n",
      "Time Comparison: 1.9793503284454346 and 1.674710750579834 seconds\n",
      "The anomaly scores are: 1851 and 2080\n",
      "\n",
      "weekly_nasdaq.csv\n",
      "2080\n",
      "Time Comparison: 1.7617137432098389 and 1.9943292140960693 seconds\n",
      "The anomaly scores are: 1844 and 2081\n",
      "\n",
      "weekly_sp500.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 138\u001b[0m\n\u001b[1;32m    130\u001b[0m rm_time_1 \u001b[38;5;241m=\u001b[39m rm_time_1 \u001b[38;5;241m+\u001b[39m (rm_end_1 \u001b[38;5;241m-\u001b[39m rm_start_1)\n\u001b[1;32m    136\u001b[0m tm_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 138\u001b[0m \u001b[43mtm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactiveColumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m tm_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    141\u001b[0m tm_time \u001b[38;5;241m=\u001b[39m tm_time \u001b[38;5;241m+\u001b[39m (tm_end \u001b[38;5;241m-\u001b[39m tm_start)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_path = pathlib.Path('../datasets/numenta')\n",
    "\n",
    "# pbar = tqdm(total=len(inputSources))\n",
    "for dataset in inputSources:\n",
    "\n",
    "    records = []\n",
    "    with open(input_path.joinpath(dataset), \"r\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        headers = next(reader)\n",
    "        next(reader)\n",
    "        next(reader)\n",
    "        for record in reader:\n",
    "            records.append(record)\n",
    "    \n",
    "    dateEncoder = DateEncoder(\n",
    "        timeOfDay= config[\"enc\"][\"time\"][\"timeOfDay\"], \n",
    "        weekend  = config[\"enc\"][\"time\"][\"weekend\"]\n",
    "    )\n",
    "\n",
    "    # config[\"enc\"][\"value\"][\"minimum\"] = min(float(r[1]) for r in records)\n",
    "    # config[\"enc\"][\"value\"][\"maximum\"] = max(float(r[1]) for r in records)\n",
    "    \n",
    "    scalarEncoderParams = RDSE_Parameters()\n",
    "    scalarEncoderParams.size = config[\"enc\"][\"value\"][\"size\"]\n",
    "    scalarEncoderParams.sparsity = config[\"enc\"][\"value\"][\"sparsity\"]\n",
    "    scalarEncoderParams.resolution = config[\"enc\"][\"value\"][\"resolution\"]\n",
    "    scalarEncoder = RDSE( scalarEncoderParams )\n",
    "    # encodingWidth = (dateEncoder.size + scalarEncoder.size)\n",
    "    encodingWidth = (scalarEncoder.size)\n",
    "\n",
    "    config['sp']['inputDimensions'] = (encodingWidth,)\n",
    "    config['sp']['potentialRadius'] = encodingWidth\n",
    "\n",
    "    sp = SpatialPooler(\n",
    "        inputDimensions = config['sp']['inputDimensions'],\n",
    "        columnDimensions = config['sp']['columnDimensions'],\n",
    "        potentialPct = config['sp']['potentialPct'],\n",
    "        potentialRadius = config['sp']['potentialRadius'],\n",
    "        globalInhibition = config['sp']['globalInhibition'],\n",
    "        localAreaDensity = config['sp']['localAreaDensity'],\n",
    "        synPermInactiveDec = config['sp']['synPermInactiveDec'],\n",
    "        synPermActiveInc = config['sp']['synPermActiveInc'],\n",
    "        synPermConnected = config['sp']['synPermConnected'],\n",
    "        boostStrength = config['sp']['boostStrength'],\n",
    "        wrapAround = config['sp']['wrapAround'],\n",
    "        seed = config['sp']['seed']\n",
    "    )\n",
    "\n",
    "    tm = TemporalMemory(\n",
    "        columnDimensions = config['sp']['columnDimensions'],\n",
    "        cellsPerColumn = config['tm']['cellsPerColumn'],\n",
    "        activationThreshold = config['tm']['activationThreshold'],\n",
    "        initialPermanence = config['tm']['initialPermanence'],\n",
    "        connectedPermanence = config['sp']['synPermConnected'],\n",
    "        minThreshold = config['tm']['minThreshold'],\n",
    "        maxNewSynapseCount = config['tm']['maxNewSynapseCount'],\n",
    "        permanenceIncrement = config['tm']['permanenceIncrement'],\n",
    "        permanenceDecrement = config['tm']['permanenceDecrement'],\n",
    "        predictedSegmentDecrement = config['tm']['predictedSegmentDecrement'],\n",
    "        maxSegmentsPerCell = config['tm']['maxSegmentsPerCell'],\n",
    "        maxSynapsesPerSegment = config['tm']['maxSynapsesPerSegment']\n",
    "    )\n",
    "\n",
    "    rm = ReflexiveMemory( sp.getColumnDimensions() )\n",
    "\n",
    "    enc_info = Metrics( [encodingWidth], 999999999)\n",
    "    sp_info = Metrics( sp.getColumnDimensions(), 999999999 )\n",
    "    tm_info = Metrics( [tm.numberOfCells()], 999999999 )\n",
    "    anomaly_history = AnomalyLikelihood(config[\"anomaly\"][\"period\"])\n",
    "\n",
    "    inputs = []\n",
    "    anomaly = []\n",
    "    anomalyProb = []\n",
    "   \n",
    "\n",
    "    print(\"\\n\"+dataset)\n",
    "    try:\n",
    "\n",
    "        # Start time for HTM with Reflexive Memory\n",
    "        start_time_with_rm = time.time()\n",
    "        tm_time = 0\n",
    "        tm_time_1 = 0\n",
    "        rm_time_1 = 0\n",
    "        \n",
    "        for count, record in enumerate(records):\n",
    "\n",
    "            # dateString = parse_date(record[0])\n",
    "            consumption = float(record[1])\n",
    "            inputs.append( consumption )\n",
    "            \n",
    "            # dateBits = dateEncoder.encode(dateString)\n",
    "            consumptionBits = scalarEncoder.encode(consumption)\n",
    "\n",
    "            # encoding = SDR( encodingWidth ).concatenate([consumptionBits, dateBits])\n",
    "            encoding = SDR( consumptionBits )\n",
    "            enc_info.addData( encoding )\n",
    "            \n",
    "            activeColumns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "            if count < config['learnRows']:\n",
    "\n",
    "                sp.compute(encoding, True, activeColumns)\n",
    "                sp_info.addData( activeColumns )\n",
    "\n",
    "                \n",
    "                tm_start_1 = time.time()\n",
    "\n",
    "                tm.compute(activeColumns, learn=True)\n",
    "                tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "                tm_end_1 = time.time()\n",
    "                tm_time_1 = tm_time_1 + (tm_end_1 - tm_start_1)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            else: \n",
    "\n",
    "                sp.compute(encoding, config['sp']['learn'], activeColumns)\n",
    "                sp_info.addData( activeColumns )\n",
    "\n",
    "              \n",
    "                rm_start_1 = time.time()\n",
    "                rm.compute(activeColumns, tm)\n",
    "                rm_end_1 = time.time()\n",
    "\n",
    "                rm_time_1 = rm_time_1 + (rm_end_1 - rm_start_1)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                tm_start = time.time()\n",
    "\n",
    "                tm.compute(activeColumns, learn=config['tm']['learn'])\n",
    "\n",
    "                tm_end = time.time()\n",
    "                tm_time = tm_time + (tm_end - tm_start)\n",
    "\n",
    "                tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "                \n",
    "            \n",
    "            anomaly.append( tm.anomaly )\n",
    "            anomalyProb.append( anomaly_history.compute(tm.anomaly) )\n",
    "        print(count)\n",
    "        # for key1, value1 in rm.pairs.items():\n",
    "        #     print(\"\\n\"+hashlib.md5(key1.encode()).hexdigest(), len(value1.items()))\n",
    "        #     for key2, value2 in value1.items():\n",
    "        #         result = hashlib.md5(key1.encode())\n",
    "        #         print(result.hexdigest(), end=' ')\n",
    "        #         result = hashlib.md5(key2.encode())\n",
    "        #         print(result.hexdigest(), end=' ')\n",
    "        #         print(value2[\"count\"], end=' ')\n",
    "        #         print(value2[\"time\"].timestamp())\n",
    "\n",
    "        # End time for HTM with Reflexive Memory\n",
    "        end_time_with_rm = time.time()\n",
    "\n",
    "        # Time taken with Reflexive Memory\n",
    "        time_with_rm = end_time_with_rm - start_time_with_rm\n",
    "       \n",
    "        # print(f\"Time taken with Reflexive Memory: {time_with_rm:.2f} seconds\")\n",
    "\n",
    "        # save the reflex memory table in the external file per dataset\n",
    "        # Data Set\n",
    "        # rm.save_to_csv(dataset, time_with_rm)\n",
    "        print(f\"Time Comparison: {rm_time_1} and {tm_time + tm_time_1} seconds\")\n",
    "        \n",
    "        # Filter out None values from rm.anomaly and anomaly\n",
    "        filtered_rm_anomaly = [x for x in rm.anomaly if x is not None]\n",
    "        filtered_tm_anomaly = [x for x in anomaly if x is not None]\n",
    "\n",
    "        # Calculate averages\n",
    "        average_rm_anomaly = sum(filtered_rm_anomaly) / len(filtered_rm_anomaly) if filtered_rm_anomaly else 0\n",
    "        average_tm_anomaly = sum(filtered_tm_anomaly) / len(filtered_tm_anomaly) if filtered_tm_anomaly else 0\n",
    "\n",
    "        # Print the results\n",
    "        print(\"The anomaly scores are:\", len(rm.anomaly), \"and\", len(filtered_tm_anomaly))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(e)\n",
    "\n",
    "    # pbar.update(1)\n",
    "# pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
