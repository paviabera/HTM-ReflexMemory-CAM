{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm.bindings.sdr import SDR, Metrics\n",
    "# from htm.encoders.scalar_encoder import ScalarEncoder, ScalarEncoderParameters\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import datetime\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "import time\n",
    "import traceback\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexiveMemory:\n",
    "  def __init__(self, dimensions):\n",
    "    self.acKey0 = None\n",
    "    self.pairs = {}\n",
    "    self.dimensions = dimensions\n",
    "    self.anomaly = []\n",
    "\n",
    "  def add(self, activeColumns):\n",
    "    acKey1 = '-'.join(map(str, activeColumns.sparse))\n",
    "    if(self.acKey0 != None):\n",
    "\n",
    "      sequence = self.pairs.get(self.acKey0, {})\n",
    "      sequence_data = sequence.get(acKey1, {\n",
    "         \"count\": 0,\n",
    "         \"time\": datetime.now()\n",
    "      })\n",
    "      sequence_data[\"count\"] = sequence_data[\"count\"] + 1\n",
    "      sequence_data[\"time\"] = datetime.now()\n",
    "\n",
    "      if self.pairs.get(self.acKey0, None) is None:\n",
    "        self.pairs[self.acKey0] = { acKey1: sequence_data }\n",
    "      else:\n",
    "        self.pairs[self.acKey0][acKey1] = sequence_data\n",
    "\n",
    "      table_size = 0\n",
    "      oldKey1 = None\n",
    "      oldKey2 = None\n",
    "      oldTime = datetime.now()\n",
    "      for key1, value1 in self.pairs.items():\n",
    "        table_size = table_size + len(value1.items())\n",
    "        for key2, value2 in value1.items():\n",
    "          if value2['time'] < oldTime:\n",
    "            oldKey1 = key1\n",
    "            oldKey2 = key2\n",
    "            oldTime = datetime.now()\n",
    "      if table_size > 99:\n",
    "         del self.pairs[oldKey1][oldKey2]\n",
    "\n",
    "    self.acKey0 = acKey1\n",
    "\n",
    "  def predict(self, activeColumns):\n",
    "    return_count = 0\n",
    "    return_sdr = None\n",
    "\n",
    "    acKey = '-'.join(map(str, activeColumns.sparse))\n",
    "    sequences = self.pairs.get(acKey, {})\n",
    "    for sequence_key, sequence_data in sequences.items():\n",
    "      if sequence_data[\"count\"] > return_count:\n",
    "        return_count = sequence_data[\"count\"]\n",
    "        return_sdr = sequence_key\n",
    "\n",
    "    if return_sdr is not None:\n",
    "      tmp_sdr = SDR( self.dimensions )\n",
    "      tmp_sdr.sparse = list(map(int, return_sdr.split('-')))\n",
    "      return_sdr = tmp_sdr\n",
    "\n",
    "    return return_count, return_sdr\n",
    "\n",
    "  # Control Unit\n",
    "  def learn(self, activeColumns1, tm):\n",
    "    pred_correct = False\n",
    "    pred_anomaly = None\n",
    "\n",
    "    if(self.acKey0 is not None):\n",
    "\n",
    "        activeColumns0 = SDR( self.dimensions )\n",
    "        activeColumns0.sparse = list(map(int, self.acKey0.split('-')))\n",
    "\n",
    "        tm.activateDendrites(True)\n",
    "        predictiveColumns = SDR( self.dimensions )\n",
    "        predictiveColumns.sparse = list(set(sorted(list(np.where(tm.getPredictiveCells().dense == 1)[0]))))\n",
    "\n",
    "        reflexiveCount, reflexiveColumns = self.predict(activeColumns0)\n",
    "        if reflexiveColumns is not None:\n",
    "            \n",
    "            pred_anomaly = 1 - np.count_nonzero((reflexiveColumns.dense & activeColumns1.dense)) / np.count_nonzero(activeColumns1.dense)\n",
    "            \n",
    "            # RM-1 SM-?\n",
    "            if activeColumns1.flatten() == reflexiveColumns.flatten():\n",
    "                pred_correct = True\n",
    "                pred_anomaly = 0\n",
    "\n",
    "            # RM-0 SM-1\n",
    "            elif activeColumns1.flatten() == predictiveColumns.flatten():\n",
    "                key1 = self.acKey0\n",
    "                key2 = '-'.join(map(str, reflexiveColumns.sparse))\n",
    "                self.pairs[key1][key2] = reflexiveCount - 5\n",
    "\n",
    "                key2 = '-'.join(map(str, predictiveColumns.sparse))\n",
    "                key2_data = self.pairs.get(key1, {}).get(key2, {\n",
    "                  \"count\": 0,\n",
    "                  \"time\": datetime.now()\n",
    "                })\n",
    "                key2_data = key2_data[\"count\"] + 1\n",
    "                self.pairs[key1][key2] = key2_data\n",
    "\n",
    "            # RM-0 SM-0\n",
    "            else:\n",
    "                key1 = self.acKey0\n",
    "                key2 = '-'.join(map(str, reflexiveColumns.sparse))\n",
    "                self.pairs[key1][key2][\"count\"] = reflexiveCount - 1\n",
    "                \n",
    "    self.anomaly.append( pred_anomaly )\n",
    "\n",
    "  def compute(self, activeColumns, tm):\n",
    "    self.learn(activeColumns, tm)\n",
    "    self.add(activeColumns)\n",
    "\n",
    "\n",
    "  def save_to_csv(self, dataset_name, save_dir='./saved_reflex_data/'):\n",
    "      # Ensure the directory exists\n",
    "      os.makedirs(save_dir, exist_ok=True)\n",
    "      \n",
    "      # Create a filename based on the dataset name, in the specified directory\n",
    "      filename = os.path.join(save_dir, f\"{dataset_name}_reflex_memory.csv\")\n",
    "      \n",
    "      # Save self.pairs to a CSV file\n",
    "      with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Key (1024 bits)', 'Values (1024 bits)'])\n",
    "          \n",
    "          # Write each key (as 1024 bits) and all corresponding values (also 1024 bits each)\n",
    "        for key1, value1 in rm.pairs.items():\n",
    "          for key2, value2 in value1.items():\n",
    "            result = hashlib.md5(key1.encode())\n",
    "            print(result.hexdigest(), end=' ')\n",
    "            result = hashlib.md5(key2.encode())\n",
    "            print(result.hexdigest(), end=' ')\n",
    "            print(value2[\"count\"], end=' ')\n",
    "            print(value2[\"time\"].timestamp())\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSources = [\n",
    "    # \"hourly_numentaTM_speed_7578.csv\",\n",
    "    # \"hourly_numentaTM_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\",\n",
    "    # \"hourly_numentaTM_exchange-3_cpc_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-3_cpm_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-2_cpc_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-2_cpm_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-4_cpc_results.csv\",\n",
    "    # \"hourly_numentaTM_exchange-4_cpm_results.csv\",\n",
    "    # \"hourly_numentaTM_rogue_agent_key_hold.csv\",\n",
    "    # \"hourly_numentaTM_TravelTime_451.csv\",\n",
    "    # \"hourly_numentaTM_occupancy_6005.csv\",\n",
    "    # \"hourly_numentaTM_speed_t4013.csv\",\n",
    "    # \"hourly_numentaTM_TravelTime_387.csv\",\n",
    "    # \"hourly_numentaTM_occupancy_t4013.csv\",\n",
    "    # \"hourly_numentaTM_speed_6005.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_flatmiddle.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_jumpsdown.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_jumpsup.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_no_noise.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_nojump.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_perfect_square_wave.csv\",\n",
    "    # \"hourly_numentaTM_art_daily_small_noise.csv\",\n",
    "    # \"hourly_numentaTM_art_flatline.csv\",\n",
    "    # \"hourly_numentaTM_art_increase_spike_density.csv\",\n",
    "    # \"hourly_numentaTM_art_load_balancer_spikes.csv\",\n",
    "    # \"hourly_numentaTM_art_noisy.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_24ae8d.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_53ea38.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_5f5533.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_77c1ca.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_825cc2.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_ac20cd.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_c6585a.csv\",\n",
    "    # \"hourly_numentaTM_ec2_cpu_utilization_fe7f93.csv\",\n",
    "    # \"hourly_numentaTM_ec2_disk_write_bytes_c0d644.csv\",\n",
    "    # \"hourly_numentaTM_ec2_network_in_257a54.csv\",\n",
    "    # \"hourly_numentaTM_ec2_request_latency_system_failure.csv\",\n",
    "    # \"hourly_numentaTM_elb_request_count_8c0756.csv\",\n",
    "    # \"hourly_numentaTM_rds_cpu_utilization_cc0c53.csv\",\n",
    "    # \"hourly_numentaTM_rds_cpu_utilization_e47b3b.csv\",\n",
    "    # \"hourly_numentaTM_grok_asg_anomaly.csv\",\n",
    "    # \"hourly_numentaTM_ec2_disk_write_bytes_1ef3de.csv\",\n",
    "    # \"hourly_numentaTM_ec2_network_in_5abac7.csv\",\n",
    "    # \"hourly_numentaTM_rogue_agent_key_updown.csv\",\n",
    "    # \"hourly_numentaTM_ambient_temperature_system_failure.csv\",\n",
    "    # \"hourly_numentaTM_nyc_taxi.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_AMZN.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_FB.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_GOOG.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_KO.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_CVS.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_PFE.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_UPS.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_IBM.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_AAPL.csv\",\n",
    "    # \"hourly_numentaTM_Twitter_volume_CRM.csv\",\n",
    "    # \"hourly_numentaTM_cpu_utilization_asg_misconfiguration.csv\",\n",
    "    # \"hourly_numentaTM_machine_temperature_system_failure.csv\",\n",
    "\n",
    "\n",
    "#    \"value1_pseudo_periodic_synthetic_1.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_2.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_3.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_4.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_5.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_6.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_7.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_8.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_9.csv\",\n",
    "#    \"value1_pseudo_periodic_synthetic_10.csv\",\n",
    "#    \"monthly_gold_prices.csv\",\n",
    "   \"monthly_sp500.csv\",\n",
    "   \"weekly_dow_jones.csv\",\n",
    "   \"weekly_nasdaq.csv\",\n",
    "   \"weekly_sp500.csv\",\n",
    "   \"monthly_vix_close.csv\",\n",
    "   \"monthly_vix_high.csv\",\n",
    "   \"monthly_vix_low.csv\",\n",
    "   \"monthly_vix_open.csv\",\n",
    "   \"daily_natural_gas.csv\",\n",
    "   \"daily_oil_prices.csv\",\n",
    "   \"value1_vix_close.csv\",\n",
    "   \"value1_vix_high.csv\",\n",
    "   \"value1_vix_low.csv\",\n",
    "   \"value1_vix_open.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_date(date_str):\n",
    "    formats = {\n",
    "        7: \"%Y-%m\",                    # Format: yyyy-mm\n",
    "        10: \"%Y-%m-%d\",                # Format: yyyy-mm-dd\n",
    "        19: \"%Y-%m-%d %H:%M:%S\"        # Format: yyyy-mm-dd hh-mm-ss\n",
    "    }\n",
    "\n",
    "    date_format = formats.get(len(date_str))\n",
    "\n",
    "    if date_format:\n",
    "        # Use `datetime.strptime` directly\n",
    "        return datetime.strptime(date_str, date_format)\n",
    "    else:\n",
    "        raise ValueError(f\"Date format not recognized for: {date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'enc': {\n",
    "        \"value\" :\n",
    "            {'resolution': 0.88, 'size': 700, 'sparsity': 0.02},\n",
    "        \"time\": \n",
    "            {'timeOfDay': (30, 1), 'weekend': 21}\n",
    "    },\n",
    "    'sp': {\n",
    "        'inputDimensions': None,\n",
    "        'columnDimensions': (1638,),\n",
    "        'potentialPct': 0.85,\n",
    "        'potentialRadius': None,\n",
    "        'globalInhibition': True,\n",
    "        'localAreaDensity': 0.04395604395604396,\n",
    "        'synPermInactiveDec': 0.006,\n",
    "        'synPermActiveInc': 0.04,\n",
    "        'synPermConnected': 0.13999999999999999,\n",
    "        'boostStrength': 3.0,\n",
    "        'wrapAround': True,\n",
    "        'seed': 1,\n",
    "        'learn': False,\n",
    "    },\n",
    "    'tm': {\n",
    "        'cellsPerColumn': 13,\n",
    "        'activationThreshold': 17,\n",
    "        'initialPermanence': 0.21,\n",
    "        'minThreshold': 10,\n",
    "        'maxNewSynapseCount': 32,\n",
    "        'permanenceIncrement': 0.1,\n",
    "        'permanenceDecrement': 0.1,\n",
    "        'predictedSegmentDecrement': 0.0,\n",
    "        'maxSegmentsPerCell': 128,\n",
    "        'maxSynapsesPerSegment': 64,\n",
    "        'learn': True\n",
    "    },\n",
    "    'anomaly': {'period': 1000},\n",
    "    'learnRows': 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 1/14 [00:01<00:19,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4539/4101604370.py\", line 80, in <module>\n",
      "    dateString = parse_date(record[0])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4539/3546427892.py\", line 14, in parse_date\n",
      "    raise ValueError(f\"Date format not recognized for: {date_str}\")\n",
      "ValueError: Date format not recognized for: 15.5\n",
      "\n",
      "Date format not recognized for: 15.5\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4539/4101604370.py\", line 80, in <module>\n",
      "    dateString = parse_date(record[0])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4539/3546427892.py\", line 14, in parse_date\n",
      "    raise ValueError(f\"Date format not recognized for: {date_str}\")\n",
      "ValueError: Date format not recognized for: 16.75\n",
      "\n",
      "Date format not recognized for: 16.75\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4539/4101604370.py\", line 80, in <module>\n",
      "    dateString = parse_date(record[0])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4539/3546427892.py\", line 14, in parse_date\n",
      "    raise ValueError(f\"Date format not recognized for: {date_str}\")\n",
      "ValueError: Date format not recognized for: 15.5\n",
      "\n",
      "Date format not recognized for: 15.5\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4539/4101604370.py\", line 80, in <module>\n",
      "    dateString = parse_date(record[0])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4539/3546427892.py\", line 14, in parse_date\n",
      "    raise ValueError(f\"Date format not recognized for: {date_str}\")\n",
      "ValueError: Date format not recognized for: 16.72\n",
      "\n",
      "Date format not recognized for: 16.72\n",
      "Time taken with Reflexive Memory: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "input_path = pathlib.Path('../datasets/numenta')\n",
    "\n",
    "pbar = tqdm(total=len(inputSources))\n",
    "for dataset in inputSources:\n",
    "\n",
    "    records = []\n",
    "    with open(input_path.joinpath(dataset), \"r\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        headers = next(reader)\n",
    "        next(reader)\n",
    "        next(reader)\n",
    "        for record in reader:\n",
    "            records.append(record)\n",
    "    \n",
    "    dateEncoder = DateEncoder(\n",
    "        timeOfDay= config[\"enc\"][\"time\"][\"timeOfDay\"], \n",
    "        weekend  = config[\"enc\"][\"time\"][\"weekend\"]\n",
    "    )\n",
    "\n",
    "    # config[\"enc\"][\"value\"][\"minimum\"] = min(float(r[1]) for r in records)\n",
    "    # config[\"enc\"][\"value\"][\"maximum\"] = max(float(r[1]) for r in records)\n",
    "    \n",
    "    scalarEncoderParams = RDSE_Parameters()\n",
    "    scalarEncoderParams.size = config[\"enc\"][\"value\"][\"size\"]\n",
    "    scalarEncoderParams.sparsity = config[\"enc\"][\"value\"][\"sparsity\"]\n",
    "    scalarEncoderParams.resolution = config[\"enc\"][\"value\"][\"resolution\"]\n",
    "    scalarEncoder = RDSE( scalarEncoderParams )\n",
    "    # encodingWidth = (dateEncoder.size + scalarEncoder.size)\n",
    "    encodingWidth = (scalarEncoder.size)\n",
    "\n",
    "    config['sp']['inputDimensions'] = (encodingWidth,)\n",
    "    config['sp']['potentialRadius'] = encodingWidth\n",
    "\n",
    "    sp = SpatialPooler(\n",
    "        inputDimensions = config['sp']['inputDimensions'],\n",
    "        columnDimensions = config['sp']['columnDimensions'],\n",
    "        potentialPct = config['sp']['potentialPct'],\n",
    "        potentialRadius = config['sp']['potentialRadius'],\n",
    "        globalInhibition = config['sp']['globalInhibition'],\n",
    "        localAreaDensity = config['sp']['localAreaDensity'],\n",
    "        synPermInactiveDec = config['sp']['synPermInactiveDec'],\n",
    "        synPermActiveInc = config['sp']['synPermActiveInc'],\n",
    "        synPermConnected = config['sp']['synPermConnected'],\n",
    "        boostStrength = config['sp']['boostStrength'],\n",
    "        wrapAround = config['sp']['wrapAround'],\n",
    "        seed = config['sp']['seed']\n",
    "    )\n",
    "\n",
    "    tm = TemporalMemory(\n",
    "        columnDimensions = config['sp']['columnDimensions'],\n",
    "        cellsPerColumn = config['tm']['cellsPerColumn'],\n",
    "        activationThreshold = config['tm']['activationThreshold'],\n",
    "        initialPermanence = config['tm']['initialPermanence'],\n",
    "        connectedPermanence = config['sp']['synPermConnected'],\n",
    "        minThreshold = config['tm']['minThreshold'],\n",
    "        maxNewSynapseCount = config['tm']['maxNewSynapseCount'],\n",
    "        permanenceIncrement = config['tm']['permanenceIncrement'],\n",
    "        permanenceDecrement = config['tm']['permanenceDecrement'],\n",
    "        predictedSegmentDecrement = config['tm']['predictedSegmentDecrement'],\n",
    "        maxSegmentsPerCell = config['tm']['maxSegmentsPerCell'],\n",
    "        maxSynapsesPerSegment = config['tm']['maxSynapsesPerSegment']\n",
    "    )\n",
    "\n",
    "    rm = ReflexiveMemory( sp.getColumnDimensions() )\n",
    "\n",
    "    enc_info = Metrics( [encodingWidth], 999999999)\n",
    "    sp_info = Metrics( sp.getColumnDimensions(), 999999999 )\n",
    "    tm_info = Metrics( [tm.numberOfCells()], 999999999 )\n",
    "    anomaly_history = AnomalyLikelihood(config[\"anomaly\"][\"period\"])\n",
    "\n",
    "    inputs = []\n",
    "    anomaly = []\n",
    "    anomalyProb = []\n",
    "\n",
    "    # Start time for HTM with Reflexive Memory\n",
    "    start_time_with_rm = time.time()\n",
    "\n",
    "    for count, record in enumerate(records):\n",
    "        try:\n",
    "            dateString = parse_date(record[0])\n",
    "            consumption = float(record[1])\n",
    "            inputs.append( consumption )\n",
    "            \n",
    "            dateBits = dateEncoder.encode(dateString)\n",
    "            consumptionBits = scalarEncoder.encode(consumption)\n",
    "\n",
    "            # encoding = SDR( encodingWidth ).concatenate([consumptionBits, dateBits])\n",
    "            encoding = SDR( consumptionBits )\n",
    "            enc_info.addData( encoding )\n",
    "            \n",
    "            activeColumns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "            if count < config['learnRows']:\n",
    "\n",
    "                sp.compute(encoding, True, activeColumns)\n",
    "                sp_info.addData( activeColumns )\n",
    "\n",
    "                tm.compute(activeColumns, learn=True)\n",
    "                tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "            else: \n",
    "\n",
    "                sp.compute(encoding, config['sp']['learn'], activeColumns)\n",
    "                sp_info.addData( activeColumns )\n",
    "                print(\"here\")\n",
    "                rm.compute(activeColumns, tm)\n",
    "\n",
    "                tm.compute(activeColumns, learn=config['tm']['learn'])\n",
    "                tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "            anomaly.append( tm.anomaly )\n",
    "            anomalyProb.append( anomaly_history.compute(tm.anomaly) )\n",
    "\n",
    "\n",
    "        \n",
    "            print(dataset)\n",
    "            print(len(rm.pairs.items()))\n",
    "            for key1, value1 in rm.pairs.items():\n",
    "                print(key1, len(value1.items()))\n",
    "                for key2, value2 in value1.items():\n",
    "                    result = hashlib.md5(key1.encode())\n",
    "                    print(result.hexdigest(), end=' ')\n",
    "                    result = hashlib.md5(key2.encode())\n",
    "                    print(result.hexdigest(), end=' ')\n",
    "                    print(value2[\"count\"], end=' ')\n",
    "                    print(value2[\"time\"].timestamp())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "            print(e)\n",
    "\n",
    "        # save the reflex memory table in the external file per dataset\n",
    "        # Data Set\n",
    "        # rm.save_to_csv(dataset)\n",
    "\n",
    "        pbar.update(1)\n",
    "        break\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # End time for HTM with Reflexive Memory\n",
    "    end_time_with_rm = time.time()\n",
    "\n",
    "    # Time taken with Reflexive Memory\n",
    "    time_with_rm = end_time_with_rm - start_time_with_rm\n",
    "    print(f\"Time taken with Reflexive Memory: {time_with_rm:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
