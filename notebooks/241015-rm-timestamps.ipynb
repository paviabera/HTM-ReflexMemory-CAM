{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm.bindings.sdr import SDR, Metrics\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import datetime\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "import time\n",
    "import traceback\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexiveMemory:\n",
    "  def __init__(self, dimensions, reflexSize):\n",
    "    self.acKey0 = None\n",
    "    self.pairs = {}\n",
    "    self.dimensions = dimensions\n",
    "    self.anomaly = []\n",
    "    self.enableLearn = False\n",
    "    self.tableSize  = reflexSize\n",
    "\n",
    "  def add(self, activeColumns):\n",
    "    acKey1 = '-'.join(map(str, activeColumns.sparse))\n",
    "    if(self.acKey0 != None):\n",
    "\n",
    "      sequence = self.pairs.get(self.acKey0, {})\n",
    "      sequence_data = sequence.get(acKey1, {\n",
    "         \"count\": 0,\n",
    "         \"time\": datetime.now()\n",
    "      })\n",
    "      sequence_data[\"count\"] = sequence_data[\"count\"] + 1\n",
    "      sequence_data[\"time\"] = datetime.now()\n",
    "\n",
    "      if self.pairs.get(self.acKey0, None) is None:\n",
    "        self.pairs[self.acKey0] = { acKey1: sequence_data }\n",
    "      else:\n",
    "        self.pairs[self.acKey0][acKey1] = sequence_data\n",
    "\n",
    "      table_entries = 0\n",
    "      oldKey1 = None\n",
    "      oldKey2 = None\n",
    "      oldTime = datetime.now()\n",
    "      for key1, value1 in self.pairs.items():\n",
    "        table_entries = table_entries + len(value1.items())\n",
    "        for key2, value2 in value1.items():\n",
    "          if value2['time'] < oldTime:\n",
    "            oldKey1 = key1\n",
    "            oldKey2 = key2\n",
    "            oldTime = datetime.now()\n",
    "      if table_entries > self.tableSize:\n",
    "        self.enableLearn = True\n",
    "        del self.pairs[oldKey1][oldKey2]\n",
    "        if len(self.pairs[oldKey1].items()) == 0:\n",
    "          del self.pairs[oldKey1]\n",
    "\n",
    "    self.acKey0 = acKey1\n",
    "\n",
    "  def predict(self, activeColumns):\n",
    "    return_count = 0\n",
    "    return_sdr = None\n",
    "\n",
    "    acKey = '-'.join(map(str, activeColumns.sparse))\n",
    "    sequences = self.pairs.get(acKey, {})\n",
    "    for sequence_key, sequence_data in sequences.items():\n",
    "      if sequence_data[\"count\"] > return_count:\n",
    "        return_count = sequence_data[\"count\"]\n",
    "        return_sdr = sequence_key\n",
    "\n",
    "    if return_sdr is not None:\n",
    "      tmp_sdr = SDR( self.dimensions )\n",
    "      tmp_sdr.sparse = list(map(int, return_sdr.split('-')))\n",
    "      return_sdr = tmp_sdr\n",
    "    else:\n",
    "      return_count = None\n",
    "\n",
    "    return return_count, return_sdr\n",
    "\n",
    "  # Control Unit\n",
    "  def learn(self, activeColumns1, tm):\n",
    "    pred_anomaly = None\n",
    "\n",
    "    activeColumns0 = SDR( self.dimensions )\n",
    "    activeColumns0.sparse = list(map(int, self.acKey0.split('-')))\n",
    "\n",
    "    tm.activateDendrites(True)\n",
    "    predictiveColumns = SDR( self.dimensions )\n",
    "    predictiveColumns.sparse = list(set(sorted(list(np.where(tm.getPredictiveCells().dense == 1)[0]))))\n",
    "\n",
    "    reflexiveCount, reflexiveColumns = self.predict(activeColumns0)\n",
    "    if reflexiveColumns is None:\n",
    "        reflexiveColumns = SDR( self.dimensions )\n",
    "        \n",
    "    pred_anomaly = 1 - np.count_nonzero((reflexiveColumns.dense & activeColumns1.dense)) / np.count_nonzero(activeColumns1.dense)\n",
    "    self.anomaly.append( pred_anomaly )\n",
    "\n",
    "    if self.enableLearn:\n",
    "\n",
    "      # RM-1 SM-?\n",
    "      if activeColumns1.flatten() == reflexiveColumns.flatten():\n",
    "          pred_anomaly = 0\n",
    "\n",
    "      # RM-0 SM-1\n",
    "      elif activeColumns1.flatten() == predictiveColumns.flatten():\n",
    "        if reflexiveCount is not None:\n",
    "          key1 = self.acKey0\n",
    "          key2 = '-'.join(map(str, reflexiveColumns.sparse))\n",
    "          reflexiveCount = reflexiveCount - 1\n",
    "          reflexiveCount = 1 if reflexiveCount < 1 else reflexiveCount\n",
    "          self.pairs[key1][key2][\"count\"] = reflexiveCount\n",
    "\n",
    "        key1 = self.acKey0\n",
    "        if self.pairs.get(key1, None) is None:\n",
    "          self.pairs[key1] = {}\n",
    "        key2 = '-'.join(map(str, predictiveColumns.sparse))\n",
    "        key2_data = self.pairs[key1].get(key2, {\n",
    "          \"count\": 1,\n",
    "          \"time\": datetime.now()\n",
    "        })\n",
    "        self.pairs[key1][key2] = key2_data\n",
    "\n",
    "      # RM-0 SM-0\n",
    "      else:\n",
    "        if reflexiveCount is not None:\n",
    "          key1 = self.acKey0\n",
    "          key2 = '-'.join(map(str, reflexiveColumns.sparse))\n",
    "          reflexiveCount = reflexiveCount - 1\n",
    "          reflexiveCount = 1 if reflexiveCount < 1 else reflexiveCount\n",
    "          self.pairs[key1][key2][\"count\"] = reflexiveCount\n",
    "\n",
    "\n",
    "  def compute(self, activeColumns, tm):\n",
    "    if self.acKey0 is not None:\n",
    "      self.learn(activeColumns, tm)  \n",
    "    self.add(activeColumns)\n",
    "  \n",
    "  def save_to_csv(self, dataset_name, time_with_rm):\n",
    "\n",
    "    save_dir='./saved_reflex_data/'\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a filename based on the dataset name, in the specified directory\n",
    "    filename = os.path.join(save_dir, \"Sabrina_4CAM_delay_times.xlsx\")\n",
    "  \n",
    "    # filename = os.path.join(save_dir, f\"{dataset_name}_reflex_memory.csv\")\n",
    "    \n",
    "    # # Save self.pairs to a CSV file\n",
    "    # with open(filename, 'w', newline='') as f:\n",
    "    #   writer = csv.writer(f)\n",
    "    #   # writer.writerow(['Key (1024 bits)', 'Values (1024 bits)'])\n",
    "        \n",
    "    #     # Write each key (as 1024 bits) and all corresponding values (also 1024 bits each)\n",
    "    # print(\"The name of the dataset: \",dataset_name)  \n",
    "    # print(\"No of keys: \", len(self.pairs.items()))\n",
    "    valueCount = []\n",
    "    frequencyCount = []\n",
    "    for key1, value1 in self.pairs.items():\n",
    "        valueCount.append(len(value1.items()))\n",
    "        \n",
    "        # print(\"\\n\"+hashlib.md5(key1.encode()).hexdigest(), len(value1.items()))\n",
    "        for key2, value2 in value1.items():\n",
    "            result = hashlib.md5(key1.encode())\n",
    "            # print(result.hexdigest(), end=' ')\n",
    "            result = hashlib.md5(key2.encode())\n",
    "            # print(result.hexdigest(), end=' ')\n",
    "            # print(value2[\"count\"], end=' ')\n",
    "            # print(value2[\"time\"].timestamp())\n",
    "            frequencyCount.append(value2[\"count\"])\n",
    "\n",
    "    # print(\"Values count per key: \",valueCount)\n",
    "    # print(\"Highest Value count: \", max(valueCount))\n",
    "    # print(\"Frequency count: \", frequencyCount)\n",
    "    # print(f\"Time taken with Reflexive Memory: {time_with_rm:.2f} seconds\")\n",
    "    # print(\"length\",len(frequencyCount))\n",
    "    \n",
    "\n",
    "    \n",
    "    # Data to be written into the Excel file\n",
    "    data = {\n",
    "        \"Dataset Name\": [dataset_name],\n",
    "        \"No of Keys\": [len(self.pairs.items())],\n",
    "        \"Values Count per Key\": [valueCount],\n",
    "        \"Highest Value Count\": [max(valueCount)],\n",
    "        \"Frequency count\": [frequencyCount],\n",
    "        \"Time Taken with Reflexive Memory (seconds)\": [time_with_rm]\n",
    "    }\n",
    "\n",
    "    # Convert the new data into a pandas DataFrame\n",
    "    new_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Check if the Excel file already exists\n",
    "    if os.path.exists(filename):\n",
    "        # If the file exists, load the existing data\n",
    "        existing_df = pd.read_excel(filename)\n",
    "        # Append the new data to the existing DataFrame\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        # If the file does not exist, the new data is the only data\n",
    "        updated_df = new_df\n",
    "\n",
    "    # Write the updated DataFrame to the Excel file\n",
    "    updated_df.to_excel(filename, index=False)\n",
    "\n",
    "    print(\"Data written to\", filename)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSources = [\n",
    "   \"monthly_sp500.csv\",\n",
    "   \"weekly_dow_jones.csv\",\n",
    "   \"weekly_nasdaq.csv\",\n",
    "   \"weekly_sp500.csv\",\n",
    "   \"monthly_vix_close.csv\",\n",
    "   \"monthly_vix_high.csv\",\n",
    "   \"monthly_vix_low.csv\",\n",
    "   \"monthly_vix_open.csv\",\n",
    "   \"daily_natural_gas.csv\",\n",
    "   \"daily_oil_prices.csv\",\n",
    "   \"value1_vix_close.csv\",\n",
    "   \"value1_vix_high.csv\",\n",
    "   \"value1_vix_low.csv\",\n",
    "   \"value1_vix_open.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'enc': {\n",
    "        \"value\" :\n",
    "            {'resolution': 0.88, 'size': 700, 'sparsity': 0.02},\n",
    "        \"time\": \n",
    "            {'timeOfDay': (30, 1), 'weekend': 21}\n",
    "    },\n",
    "    'sp': {\n",
    "        'inputDimensions': None,\n",
    "        'columnDimensions': (1638,),\n",
    "        'potentialPct': 0.85,\n",
    "        'potentialRadius': None,\n",
    "        'globalInhibition': True,\n",
    "        'localAreaDensity': 0.04395604395604396,\n",
    "        'synPermInactiveDec': 0.006,\n",
    "        'synPermActiveInc': 0.04,\n",
    "        'synPermConnected': 0.13999999999999999,\n",
    "        'boostStrength': 3.0,\n",
    "        'wrapAround': True,\n",
    "        'seed': 1,\n",
    "        'learn': False,\n",
    "    },\n",
    "    'tm': {\n",
    "        'cellsPerColumn': 13,\n",
    "        'activationThreshold': 17,\n",
    "        'initialPermanence': 0.21,\n",
    "        'minThreshold': 10,\n",
    "        'maxNewSynapseCount': 32,\n",
    "        'permanenceIncrement': 0.1,\n",
    "        'permanenceDecrement': 0.1,\n",
    "        'predictedSegmentDecrement': 0.0,\n",
    "        'maxSegmentsPerCell': 128,\n",
    "        'maxSynapsesPerSegment': 64,\n",
    "        'learn': True\n",
    "    },\n",
    "    'anomaly': {'period': 1000},\n",
    "    'learnRows': 100,\n",
    "    'reflexSize': 128\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monthly_sp500.csv\n",
      "Data Points: 1765\n",
      "Time Comparison: 0.3349621295928955 and 0.28084635734558105 seconds\n",
      "The anomaly scores are: 0.4608191524858192 and 0.31068327678799934\n",
      "Anomaly score samples: 1665 and 1766\n",
      "\n",
      "weekly_dow_jones.csv\n",
      "Data Points: 2079\n",
      "Time Comparison: 1.1966047286987305 and 0.6025938987731934 seconds\n",
      "The anomaly scores are: 0.9886516759306057 and 0.9404447105887812\n",
      "Anomaly score samples: 1979 and 2080\n",
      "\n",
      "weekly_nasdaq.csv\n",
      "Data Points: 2080\n",
      "Time Comparison: 1.0501396656036377 and 0.665989875793457 seconds\n",
      "The anomaly scores are: 0.9226010101010111 and 0.778625393219217\n",
      "Anomaly score samples: 1980 and 2081\n",
      "\n",
      "weekly_sp500.csv\n",
      "Data Points: 2081\n",
      "Time Comparison: 0.936962366104126 and 0.6273539066314697 seconds\n",
      "The anomaly scores are: 0.8627096303774751 and 0.6528644989437883\n",
      "Anomaly score samples: 1981 and 2082\n",
      "\n",
      "monthly_vix_close.csv\n",
      "Data Points: 4050\n",
      "Time Comparison: 1.3510453701019287 and 1.45729660987854 seconds\n",
      "The anomaly scores are: 0.3870112517580893 and 0.09763707184690042\n",
      "Anomaly score samples: 3950 and 4051\n",
      "\n",
      "monthly_vix_high.csv\n",
      "Data Points: 4050\n",
      "Time Comparison: 1.2826495170593262 and 1.198939561843872 seconds\n",
      "The anomaly scores are: 0.38109001406469534 and 0.10484722568404083\n",
      "Anomaly score samples: 3950 and 4051\n",
      "\n",
      "monthly_vix_low.csv\n",
      "Data Points: 4050\n",
      "Time Comparison: 1.1414291858673096 and 0.9553184509277344 seconds\n",
      "The anomaly scores are: 0.3301793248945122 and 0.07653117216332654\n",
      "Anomaly score samples: 3950 and 4051\n",
      "\n",
      "monthly_vix_open.csv\n",
      "Data Points: 4050\n",
      "Time Comparison: 1.2826564311981201 and 1.2106592655181885 seconds\n",
      "The anomaly scores are: 0.38757735583684905 and 0.08889437460054794\n",
      "Anomaly score samples: 3950 and 4051\n",
      "\n",
      "daily_natural_gas.csv\n",
      "Data Points: 5798\n",
      "Time Comparison: 0.8965997695922852 and 0.33344244956970215 seconds\n",
      "The anomaly scores are: 0.06282662532662493 and 0.01805148399215764\n",
      "Anomaly score samples: 5698 and 5799\n",
      "\n",
      "daily_oil_prices.csv\n",
      "Data Points: 8300\n",
      "Time Comparison: 2.0044057369232178 and 0.9876139163970947 seconds\n",
      "The anomaly scores are: 0.2287076558265606 and 0.027968517875658103\n",
      "Anomaly score samples: 8200 and 8301\n",
      "\n",
      "value1_vix_close.csv\n",
      "Data Points: 4049\n",
      "Time Comparison: 1.2778851985931396 and 1.275221347808838 seconds\n",
      "The anomaly scores are: 0.39185729157873356 and 0.10089163246231131\n",
      "Anomaly score samples: 3949 and 4050\n",
      "\n",
      "value1_vix_high.csv\n",
      "Data Points: 4049\n",
      "Time Comparison: 1.3296833038330078 and 1.295623540878296 seconds\n",
      "The anomaly scores are: 0.3790622098421548 and 0.1051165980969866\n",
      "Anomaly score samples: 3949 and 4050\n",
      "\n",
      "value1_vix_low.csv\n",
      "Data Points: 4049\n",
      "Time Comparison: 1.0975215435028076 and 0.9332718849182129 seconds\n",
      "The anomaly scores are: 0.33162403984131067 and 0.0768106997029189\n",
      "Anomaly score samples: 3949 and 4050\n",
      "\n",
      "value1_vix_open.csv\n",
      "Data Points: 4049\n",
      "Time Comparison: 1.3161993026733398 and 1.600283145904541 seconds\n",
      "The anomaly scores are: 0.36876072704762924 and 0.08383401920306094\n",
      "Anomaly score samples: 3949 and 4050\n"
     ]
    }
   ],
   "source": [
    "input_path = pathlib.Path('../datasets/numenta')\n",
    "\n",
    "for dataset in inputSources:\n",
    "\n",
    "    records = []\n",
    "    with open(input_path.joinpath(dataset), \"r\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        headers = next(reader)\n",
    "        next(reader)\n",
    "        next(reader)\n",
    "        for record in reader:\n",
    "            records.append(record)\n",
    "        \n",
    "    scalarEncoderParams = RDSE_Parameters()\n",
    "    scalarEncoderParams.size = config[\"enc\"][\"value\"][\"size\"]\n",
    "    scalarEncoderParams.sparsity = config[\"enc\"][\"value\"][\"sparsity\"]\n",
    "    scalarEncoderParams.resolution = config[\"enc\"][\"value\"][\"resolution\"]\n",
    "    scalarEncoder = RDSE( scalarEncoderParams )\n",
    "    encodingWidth = (scalarEncoder.size)\n",
    "\n",
    "    config['sp']['inputDimensions'] = (encodingWidth,)\n",
    "    config['sp']['potentialRadius'] = encodingWidth\n",
    "\n",
    "    sp = SpatialPooler(\n",
    "        inputDimensions = config['sp']['inputDimensions'],\n",
    "        columnDimensions = config['sp']['columnDimensions'],\n",
    "        potentialPct = config['sp']['potentialPct'],\n",
    "        potentialRadius = config['sp']['potentialRadius'],\n",
    "        globalInhibition = config['sp']['globalInhibition'],\n",
    "        localAreaDensity = config['sp']['localAreaDensity'],\n",
    "        synPermInactiveDec = config['sp']['synPermInactiveDec'],\n",
    "        synPermActiveInc = config['sp']['synPermActiveInc'],\n",
    "        synPermConnected = config['sp']['synPermConnected'],\n",
    "        boostStrength = config['sp']['boostStrength'],\n",
    "        wrapAround = config['sp']['wrapAround'],\n",
    "        seed = config['sp']['seed']\n",
    "    )\n",
    "\n",
    "    tm = TemporalMemory(\n",
    "        columnDimensions = config['sp']['columnDimensions'],\n",
    "        cellsPerColumn = config['tm']['cellsPerColumn'],\n",
    "        activationThreshold = config['tm']['activationThreshold'],\n",
    "        initialPermanence = config['tm']['initialPermanence'],\n",
    "        connectedPermanence = config['sp']['synPermConnected'],\n",
    "        minThreshold = config['tm']['minThreshold'],\n",
    "        maxNewSynapseCount = config['tm']['maxNewSynapseCount'],\n",
    "        permanenceIncrement = config['tm']['permanenceIncrement'],\n",
    "        permanenceDecrement = config['tm']['permanenceDecrement'],\n",
    "        predictedSegmentDecrement = config['tm']['predictedSegmentDecrement'],\n",
    "        maxSegmentsPerCell = config['tm']['maxSegmentsPerCell'],\n",
    "        maxSynapsesPerSegment = config['tm']['maxSynapsesPerSegment']\n",
    "    )\n",
    "\n",
    "    rm = ReflexiveMemory( sp.getColumnDimensions(), config['reflexSize'] )\n",
    "\n",
    "    enc_info = Metrics( [encodingWidth], 999999999)\n",
    "    sp_info = Metrics( sp.getColumnDimensions(), 999999999 )\n",
    "    tm_info = Metrics( [tm.numberOfCells()], 999999999 )\n",
    "    anomaly_history = AnomalyLikelihood(config[\"anomaly\"][\"period\"])\n",
    "\n",
    "    inputs = []\n",
    "    anomaly = []\n",
    "    anomalyProb = []\n",
    "   \n",
    "    print(\"\\n\"+dataset)\n",
    "    try:\n",
    "\n",
    "        # Start time for HTM with Reflexive Memory\n",
    "        start_time_with_rm = time.time()\n",
    "        tm_time = 0\n",
    "        tm_time_1 = 0\n",
    "        rm_time_1 = 0\n",
    "        \n",
    "        for count, record in enumerate(records):\n",
    "\n",
    "            consumption = float(record[1])\n",
    "\n",
    "            inputs.append( consumption )\n",
    "            consumptionBits = scalarEncoder.encode(consumption)\n",
    "\n",
    "            encoding = SDR( consumptionBits )\n",
    "            enc_info.addData( encoding )\n",
    "            \n",
    "            activeColumns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "            if count < config['learnRows']:\n",
    "\n",
    "                sp.compute(encoding, True, activeColumns)\n",
    "                sp_info.addData( activeColumns )\n",
    "\n",
    "                timestamp = time.time()\n",
    "                tm.compute(activeColumns, learn=True)\n",
    "                tm_time_1 = tm_time_1 + (time.time() - timestamp)\n",
    "\n",
    "                tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "            else: \n",
    "\n",
    "                sp.compute(encoding, config['sp']['learn'], activeColumns)\n",
    "                sp_info.addData( activeColumns )\n",
    "              \n",
    "                timestamp = time.time()\n",
    "                rm.compute(activeColumns, tm)\n",
    "                rm_time_1 = rm_time_1 + (time.time() - timestamp)\n",
    "\n",
    "                timestamp = time.time()\n",
    "                tm.compute(activeColumns, learn=config['tm']['learn'])\n",
    "                tm_time = tm_time + (time.time() - timestamp)\n",
    "\n",
    "                tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "            anomaly.append( tm.anomaly )\n",
    "            anomalyProb.append( anomaly_history.compute(tm.anomaly) )\n",
    "\n",
    "        print(\"Data Points:\",count)\n",
    "\n",
    "        # End time for HTM with Reflexive Memory\n",
    "        end_time_with_rm = time.time()\n",
    "\n",
    "        # Time taken with Reflexive Memory\n",
    "        time_with_rm = end_time_with_rm - start_time_with_rm\n",
    "       \n",
    "        # print(f\"Time taken with Reflexive Memory: {time_with_rm:.2f} seconds\")\n",
    "\n",
    "        # save the reflex memory table in the external file per dataset\n",
    "        # Data Set\n",
    "        # rm.save_to_csv(dataset, time_with_rm)\n",
    "        print(f\"Time Comparison: {rm_time_1} and {tm_time + tm_time_1} seconds\")\n",
    "        \n",
    "        # Filter out None values from rm.anomaly and anomaly\n",
    "        filtered_rm_anomaly = [x for x in rm.anomaly if x is not None]\n",
    "\n",
    "        # Calculate averages\n",
    "        average_rm_anomaly = sum(rm.anomaly) / len(rm.anomaly)\n",
    "        average_tm_anomaly = sum(anomaly) / len(anomaly)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"The anomaly scores are:\", average_rm_anomaly, \"and\", average_tm_anomaly)\n",
    "        print(\"Anomaly score samples:\", len(rm.anomaly), \"and\", len(anomaly))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
